services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    ports:
      - "9000:9000"
    environment:
      - DOC_PIPELINE_REDIS_URL=redis://redis:6379/0
      - DOC_PIPELINE_API_PORT=9000
      - DOC_PIPELINE_LOG_JSON=true
      - DOC_PIPELINE_LOG_LEVEL=INFO
      # Redis pool settings
      - DOC_PIPELINE_REDIS_MAX_CONNECTIONS=${DOC_PIPELINE_REDIS_MAX_CONNECTIONS:-500}
      # Rate limiting
      - DOC_PIPELINE_RATE_LIMIT_ENABLED=${DOC_PIPELINE_RATE_LIMIT_ENABLED:-true}
      - DOC_PIPELINE_RATE_LIMIT_REQUESTS=${DOC_PIPELINE_RATE_LIMIT_REQUESTS:-30}
      - DOC_PIPELINE_RATE_LIMIT_WINDOW=${DOC_PIPELINE_RATE_LIMIT_WINDOW:-second}
      # Auth settings (optional)
      - DOC_PIPELINE_API_KEY=${DOC_PIPELINE_API_KEY:-}
      - DOC_PIPELINE_DATABASE_URL=${DOC_PIPELINE_DATABASE_URL:-}
      # Warmup endpoint API key (optional)
      - DOC_PIPELINE_WARMUP_API_KEY=${DOC_PIPELINE_WARMUP_API_KEY:-}
      # Sentry / GlitchTip
      - DOC_PIPELINE_SENTRY_DSN=${DOC_PIPELINE_SENTRY_DSN:-}
    volumes:
      # Shared temp directory for images between API and worker
      - temp-images:/tmp/doc-pipeline
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Worker DocID 1 - sempre ativo (default)
  worker-docid-1:
    build:
      context: .
      dockerfile: Dockerfile.worker-docid
    container_name: doc-pipeline-worker-docid-1
    ports:
      - "9010:9010"
    environment:
      - WORKER_ID=docid-1
      - DOC_PIPELINE_REDIS_URL=redis://redis:6379/0
      - DOC_PIPELINE_WARMUP_ON_START=true
      - DOC_PIPELINE_LOG_JSON=true
      - DOC_PIPELINE_LOG_LEVEL=INFO
      - DOC_PIPELINE_WORKER_HEALTH_PORT=9010
      - DOC_PIPELINE_CLASSIFIER_MODEL_PATH=/app/models/classifier.pth
      - DOC_PIPELINE_EXTRACTOR_BACKEND=${DOC_PIPELINE_EXTRACTOR_BACKEND:-qwen-vl}
      - DOC_PIPELINE_EXTRACTOR_MODEL_QWEN=${DOC_PIPELINE_EXTRACTOR_MODEL_QWEN:-Qwen/Qwen2.5-VL-3B-Instruct}
      - DOC_PIPELINE_CLASSIFIER_DEVICE=cuda:0
      - DOC_PIPELINE_EXTRACTOR_DEVICE=cuda:0
      - DOC_PIPELINE_INFERENCE_SERVER_ENABLED=${DOC_PIPELINE_INFERENCE_SERVER_ENABLED:-false}
      - DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS=${DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS:-30}
      - DOC_PIPELINE_WORKER_CONCURRENT_JOBS=${DOC_PIPELINE_WORKER_CONCURRENT_JOBS:-4}
      - HF_TOKEN=${HF_TOKEN:-}
      - DOC_PIPELINE_SENTRY_DSN=${DOC_PIPELINE_SENTRY_DSN:-}
    volumes:
      - temp-images:/tmp/doc-pipeline
      - model-cache:/root/.cache/huggingface
      - ./models:/app/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    restart: unless-stopped

  # Worker DocID 2 - sempre ativo
  worker-docid-2:
    build:
      context: .
      dockerfile: Dockerfile.worker-docid
    container_name: doc-pipeline-worker-docid-2
    ports:
      - "9012:9010"
    environment:
      - WORKER_ID=docid-2
      - DOC_PIPELINE_REDIS_URL=redis://redis:6379/0
      - DOC_PIPELINE_WARMUP_ON_START=true
      - DOC_PIPELINE_LOG_JSON=true
      - DOC_PIPELINE_LOG_LEVEL=INFO
      - DOC_PIPELINE_WORKER_HEALTH_PORT=9010
      - DOC_PIPELINE_CLASSIFIER_MODEL_PATH=/app/models/classifier.pth
      - DOC_PIPELINE_EXTRACTOR_BACKEND=${DOC_PIPELINE_EXTRACTOR_BACKEND:-qwen-vl}
      - DOC_PIPELINE_EXTRACTOR_MODEL_QWEN=${DOC_PIPELINE_EXTRACTOR_MODEL_QWEN:-Qwen/Qwen2.5-VL-3B-Instruct}
      - DOC_PIPELINE_CLASSIFIER_DEVICE=cuda:0
      - DOC_PIPELINE_EXTRACTOR_DEVICE=cuda:0
      - DOC_PIPELINE_INFERENCE_SERVER_ENABLED=${DOC_PIPELINE_INFERENCE_SERVER_ENABLED:-false}
      - DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS=${DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS:-30}
      - DOC_PIPELINE_WORKER_CONCURRENT_JOBS=${DOC_PIPELINE_WORKER_CONCURRENT_JOBS:-4}
      - HF_TOKEN=${HF_TOKEN:-}
      - DOC_PIPELINE_SENTRY_DSN=${DOC_PIPELINE_SENTRY_DSN:-}
    volumes:
      - temp-images:/tmp/doc-pipeline
      - model-cache:/root/.cache/huggingface
      - ./models:/app/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    restart: unless-stopped

  # Worker DocID 3 - sempre ativo
  worker-docid-3:
    build:
      context: .
      dockerfile: Dockerfile.worker-docid
    container_name: doc-pipeline-worker-docid-3
    ports:
      - "9014:9010"
    environment:
      - WORKER_ID=docid-3
      - DOC_PIPELINE_REDIS_URL=redis://redis:6379/0
      - DOC_PIPELINE_WARMUP_ON_START=true
      - DOC_PIPELINE_LOG_JSON=true
      - DOC_PIPELINE_LOG_LEVEL=INFO
      - DOC_PIPELINE_WORKER_HEALTH_PORT=9010
      - DOC_PIPELINE_CLASSIFIER_MODEL_PATH=/app/models/classifier.pth
      - DOC_PIPELINE_EXTRACTOR_BACKEND=${DOC_PIPELINE_EXTRACTOR_BACKEND:-qwen-vl}
      - DOC_PIPELINE_EXTRACTOR_MODEL_QWEN=${DOC_PIPELINE_EXTRACTOR_MODEL_QWEN:-Qwen/Qwen2.5-VL-3B-Instruct}
      - DOC_PIPELINE_CLASSIFIER_DEVICE=cuda:0
      - DOC_PIPELINE_EXTRACTOR_DEVICE=cuda:0
      - DOC_PIPELINE_INFERENCE_SERVER_ENABLED=${DOC_PIPELINE_INFERENCE_SERVER_ENABLED:-false}
      - DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS=${DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS:-30}
      - DOC_PIPELINE_WORKER_CONCURRENT_JOBS=${DOC_PIPELINE_WORKER_CONCURRENT_JOBS:-4}
      - HF_TOKEN=${HF_TOKEN:-}
      - DOC_PIPELINE_SENTRY_DSN=${DOC_PIPELINE_SENTRY_DSN:-}
    volumes:
      - temp-images:/tmp/doc-pipeline
      - model-cache:/root/.cache/huggingface
      - ./models:/app/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    restart: unless-stopped

  # Worker DocID 4 - sempre ativo
  worker-docid-4:
    build:
      context: .
      dockerfile: Dockerfile.worker-docid
    container_name: doc-pipeline-worker-docid-4
    ports:
      - "9016:9010"
    environment:
      - WORKER_ID=docid-4
      - DOC_PIPELINE_REDIS_URL=redis://redis:6379/0
      - DOC_PIPELINE_WARMUP_ON_START=true
      - DOC_PIPELINE_LOG_JSON=true
      - DOC_PIPELINE_LOG_LEVEL=INFO
      - DOC_PIPELINE_WORKER_HEALTH_PORT=9010
      - DOC_PIPELINE_CLASSIFIER_MODEL_PATH=/app/models/classifier.pth
      - DOC_PIPELINE_EXTRACTOR_BACKEND=${DOC_PIPELINE_EXTRACTOR_BACKEND:-qwen-vl}
      - DOC_PIPELINE_EXTRACTOR_MODEL_QWEN=${DOC_PIPELINE_EXTRACTOR_MODEL_QWEN:-Qwen/Qwen2.5-VL-3B-Instruct}
      - DOC_PIPELINE_CLASSIFIER_DEVICE=cuda:0
      - DOC_PIPELINE_EXTRACTOR_DEVICE=cuda:0
      - DOC_PIPELINE_INFERENCE_SERVER_ENABLED=${DOC_PIPELINE_INFERENCE_SERVER_ENABLED:-false}
      - DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS=${DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS:-30}
      - DOC_PIPELINE_WORKER_CONCURRENT_JOBS=${DOC_PIPELINE_WORKER_CONCURRENT_JOBS:-4}
      - HF_TOKEN=${HF_TOKEN:-}
      - DOC_PIPELINE_SENTRY_DSN=${DOC_PIPELINE_SENTRY_DSN:-}
    volumes:
      - temp-images:/tmp/doc-pipeline
      - model-cache:/root/.cache/huggingface
      - ./models:/app/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    restart: unless-stopped

  # Worker DocID 5 - sempre ativo
  worker-docid-5:
    build:
      context: .
      dockerfile: Dockerfile.worker-docid
    container_name: doc-pipeline-worker-docid-5
    ports:
      - "9018:9010"
    environment:
      - WORKER_ID=docid-5
      - DOC_PIPELINE_REDIS_URL=redis://redis:6379/0
      - DOC_PIPELINE_WARMUP_ON_START=true
      - DOC_PIPELINE_LOG_JSON=true
      - DOC_PIPELINE_LOG_LEVEL=INFO
      - DOC_PIPELINE_WORKER_HEALTH_PORT=9010
      - DOC_PIPELINE_CLASSIFIER_MODEL_PATH=/app/models/classifier.pth
      - DOC_PIPELINE_EXTRACTOR_BACKEND=${DOC_PIPELINE_EXTRACTOR_BACKEND:-qwen-vl}
      - DOC_PIPELINE_EXTRACTOR_MODEL_QWEN=${DOC_PIPELINE_EXTRACTOR_MODEL_QWEN:-Qwen/Qwen2.5-VL-3B-Instruct}
      - DOC_PIPELINE_CLASSIFIER_DEVICE=cuda:0
      - DOC_PIPELINE_EXTRACTOR_DEVICE=cuda:0
      - DOC_PIPELINE_INFERENCE_SERVER_ENABLED=${DOC_PIPELINE_INFERENCE_SERVER_ENABLED:-false}
      - DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS=${DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS:-30}
      - DOC_PIPELINE_WORKER_CONCURRENT_JOBS=${DOC_PIPELINE_WORKER_CONCURRENT_JOBS:-4}
      - HF_TOKEN=${HF_TOKEN:-}
      - DOC_PIPELINE_SENTRY_DSN=${DOC_PIPELINE_SENTRY_DSN:-}
    volumes:
      - temp-images:/tmp/doc-pipeline
      - model-cache:/root/.cache/huggingface
      - ./models:/app/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    restart: unless-stopped

  # Inference Server - single VLM instance shared by all workers
  inference-server:
    build:
      context: .
      dockerfile: Dockerfile.inference-server
    container_name: doc-pipeline-inference-server
    ports:
      - "9020:9020"
    environment:
      - DOC_PIPELINE_REDIS_URL=redis://redis:6379/0
      - DOC_PIPELINE_LOG_JSON=true
      - DOC_PIPELINE_LOG_LEVEL=INFO
      - DOC_PIPELINE_INFERENCE_SERVER_HEALTH_PORT=9020
      - DOC_PIPELINE_EXTRACTOR_MODEL_QWEN=${DOC_PIPELINE_EXTRACTOR_MODEL_QWEN:-Qwen/Qwen2.5-VL-3B-Instruct}
      - DOC_PIPELINE_EXTRACTOR_DEVICE=cuda:0
      - DOC_PIPELINE_INFERENCE_BATCH_SIZE=${DOC_PIPELINE_INFERENCE_BATCH_SIZE:-4}
      - DOC_PIPELINE_INFERENCE_BATCH_TIMEOUT_MS=${DOC_PIPELINE_INFERENCE_BATCH_TIMEOUT_MS:-100}
      - HF_TOKEN=${HF_TOKEN:-}
      - DOC_PIPELINE_SENTRY_DSN=${DOC_PIPELINE_SENTRY_DSN:-}
    volumes:
      - temp-images:/tmp/doc-pipeline
      - model-cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9020/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    restart: unless-stopped

  # OCR Worker - uses EasyOCR with GPU for fast Portuguese OCR
  worker-ocr:
    build:
      context: .
      dockerfile: Dockerfile.worker-ocr
    ports:
      - "9011:9011"
    environment:
      - DOC_PIPELINE_REDIS_URL=redis://redis:6379/0
      - DOC_PIPELINE_WARMUP_ON_START=true
      - DOC_PIPELINE_LOG_JSON=true
      - DOC_PIPELINE_LOG_LEVEL=INFO
      - DOC_PIPELINE_WORKER_OCR_HEALTH_PORT=9011
      - DOC_PIPELINE_OCR_LANGUAGE=pt
      - DOC_PIPELINE_OCR_USE_GPU=true
      - DOC_PIPELINE_ORIENTATION_ENABLED=${DOC_PIPELINE_ORIENTATION_ENABLED:-true}
      - DOC_PIPELINE_CLASSIFIER_DEVICE=cuda:0
      - DOC_PIPELINE_SENTRY_DSN=${DOC_PIPELINE_SENTRY_DSN:-}
    volumes:
      # Shared temp directory for files
      - temp-images:/tmp/doc-pipeline
      # EasyOCR model cache
      - easyocr-cache:/root/.EasyOCR
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9011/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

volumes:
  redis-data:
  temp-images:
  model-cache:
  easyocr-cache:

networks:
  default:
    name: doc-pipeline
