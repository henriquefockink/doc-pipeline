services:
  redis:
    image: redis:7-alpine
    ports:
      - "127.0.0.1:6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --protected-mode no --rename-command SLAVEOF "" --rename-command REPLICAOF "" --rename-command DEBUG ""
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    ports:
      - "9000:9000"
    environment:
      - DOC_PIPELINE_REDIS_URL=redis://redis:6379/0
      - DOC_PIPELINE_API_PORT=9000
      - DOC_PIPELINE_LOG_JSON=true
      - DOC_PIPELINE_LOG_LEVEL=INFO
      # Redis pool settings
      - DOC_PIPELINE_REDIS_MAX_CONNECTIONS=${DOC_PIPELINE_REDIS_MAX_CONNECTIONS:-1000}
      # Rate limiting
      - DOC_PIPELINE_RATE_LIMIT_ENABLED=${DOC_PIPELINE_RATE_LIMIT_ENABLED:-true}
      - DOC_PIPELINE_RATE_LIMIT_REQUESTS=${DOC_PIPELINE_RATE_LIMIT_REQUESTS:-30}
      - DOC_PIPELINE_RATE_LIMIT_WINDOW=${DOC_PIPELINE_RATE_LIMIT_WINDOW:-second}
      # Auth settings (optional)
      - DOC_PIPELINE_API_KEY=${DOC_PIPELINE_API_KEY:-}
      - DOC_PIPELINE_DATABASE_URL=${DOC_PIPELINE_DATABASE_URL:-}
      # Warmup endpoint API key (optional)
      - DOC_PIPELINE_WARMUP_API_KEY=${DOC_PIPELINE_WARMUP_API_KEY:-}
      # Sentry / GlitchTip
      - DOC_PIPELINE_SENTRY_DSN=${DOC_PIPELINE_SENTRY_DSN:-}
    volumes:
      # Shared temp directory for images between API and worker
      - temp-images:/tmp/doc-pipeline
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Centralized Inference Server — hosts classifier, EasyOCR, orientation, vLLM (in-process)
  inference-server:
    build:
      context: .
      dockerfile: Dockerfile.inference-server
    container_name: doc-pipeline-inference-server
    ports:
      - "9020:9020"
    environment:
      - DOC_PIPELINE_REDIS_URL=redis://redis:6379/0
      - DOC_PIPELINE_LOG_JSON=true
      - DOC_PIPELINE_LOG_LEVEL=INFO
      - DOC_PIPELINE_INFERENCE_SERVER_HEALTH_PORT=9020
      # VLM model
      - DOC_PIPELINE_EXTRACTOR_MODEL_QWEN=${DOC_PIPELINE_EXTRACTOR_MODEL_QWEN:-Qwen/Qwen2.5-VL-3B-Instruct}
      - DOC_PIPELINE_EXTRACTOR_DEVICE=cuda:0
      # Classifier model
      - DOC_PIPELINE_CLASSIFIER_MODEL_PATH=/app/models/classifier.pth
      - DOC_PIPELINE_CLASSIFIER_MODEL_TYPE=${DOC_PIPELINE_CLASSIFIER_MODEL_TYPE:-efficientnet_b0}
      - DOC_PIPELINE_CLASSIFIER_DEVICE=cuda:0
      # OCR settings
      - DOC_PIPELINE_OCR_LANGUAGE=${DOC_PIPELINE_OCR_LANGUAGE:-pt}
      - DOC_PIPELINE_OCR_USE_GPU=true
      # Orientation
      - DOC_PIPELINE_ORIENTATION_ENABLED=${DOC_PIPELINE_ORIENTATION_ENABLED:-true}
      # Batching
      - DOC_PIPELINE_INFERENCE_BATCH_SIZE=${DOC_PIPELINE_INFERENCE_BATCH_SIZE:-16}
      - DOC_PIPELINE_INFERENCE_BATCH_TIMEOUT_MS=${DOC_PIPELINE_INFERENCE_BATCH_TIMEOUT_MS:-100}
      - HF_TOKEN=${HF_TOKEN:-}
      - DOC_PIPELINE_SENTRY_DSN=${DOC_PIPELINE_SENTRY_DSN:-}
      # vLLM embedded (in-process, zero HTTP/base64 overhead)
      - DOC_PIPELINE_VLLM_EMBEDDED=${DOC_PIPELINE_VLLM_EMBEDDED:-true}
      - DOC_PIPELINE_VLLM_MODEL=${DOC_PIPELINE_VLLM_MODEL:-Qwen/Qwen2.5-VL-3B-Instruct}
      - DOC_PIPELINE_VLLM_MAX_TOKENS=${DOC_PIPELINE_VLLM_MAX_TOKENS:-1024}
      - DOC_PIPELINE_VLLM_GPU_MEMORY_UTILIZATION=${DOC_PIPELINE_VLLM_GPU_MEMORY_UTILIZATION:-0.40}
      - DOC_PIPELINE_VLLM_MAX_MODEL_LEN=${DOC_PIPELINE_VLLM_MAX_MODEL_LEN:-4096}
    volumes:
      - temp-images:/tmp/doc-pipeline
      - model-cache:/root/.cache/huggingface
      - easyocr-cache:/root/.EasyOCR
      - ./models:/app/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9020/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 360s
    restart: unless-stopped

  # Worker DocID 1 — stateless queue consumer (no GPU)
  worker-docid-1:
    build:
      context: .
      dockerfile: Dockerfile.worker
    container_name: doc-pipeline-worker-docid-1
    ports:
      - "9010:9010"
    environment:
      - WORKER_ID=docid-1
      - DOC_PIPELINE_REDIS_URL=redis://redis:6379/0
      - DOC_PIPELINE_LOG_JSON=true
      - DOC_PIPELINE_LOG_LEVEL=INFO
      - DOC_PIPELINE_WORKER_HEALTH_PORT=9010
      - DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS=${DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS:-120}
      - DOC_PIPELINE_WORKER_CONCURRENT_JOBS=${DOC_PIPELINE_WORKER_CONCURRENT_JOBS:-4}
      - DOC_PIPELINE_SENTRY_DSN=${DOC_PIPELINE_SENTRY_DSN:-}
    volumes:
      - temp-images:/tmp/doc-pipeline
    depends_on:
      redis:
        condition: service_healthy
      inference-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # Worker DocID 2 — stateless queue consumer (no GPU)
  worker-docid-2:
    build:
      context: .
      dockerfile: Dockerfile.worker
    container_name: doc-pipeline-worker-docid-2
    ports:
      - "9012:9010"
    environment:
      - WORKER_ID=docid-2
      - DOC_PIPELINE_REDIS_URL=redis://redis:6379/0
      - DOC_PIPELINE_LOG_JSON=true
      - DOC_PIPELINE_LOG_LEVEL=INFO
      - DOC_PIPELINE_WORKER_HEALTH_PORT=9010
      - DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS=${DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS:-120}
      - DOC_PIPELINE_WORKER_CONCURRENT_JOBS=${DOC_PIPELINE_WORKER_CONCURRENT_JOBS:-4}
      - DOC_PIPELINE_SENTRY_DSN=${DOC_PIPELINE_SENTRY_DSN:-}
    volumes:
      - temp-images:/tmp/doc-pipeline
    depends_on:
      redis:
        condition: service_healthy
      inference-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # Worker DocID 3 — stateless queue consumer (no GPU)
  worker-docid-3:
    build:
      context: .
      dockerfile: Dockerfile.worker
    container_name: doc-pipeline-worker-docid-3
    ports:
      - "9014:9010"
    environment:
      - WORKER_ID=docid-3
      - DOC_PIPELINE_REDIS_URL=redis://redis:6379/0
      - DOC_PIPELINE_LOG_JSON=true
      - DOC_PIPELINE_LOG_LEVEL=INFO
      - DOC_PIPELINE_WORKER_HEALTH_PORT=9010
      - DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS=${DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS:-120}
      - DOC_PIPELINE_WORKER_CONCURRENT_JOBS=${DOC_PIPELINE_WORKER_CONCURRENT_JOBS:-4}
      - DOC_PIPELINE_SENTRY_DSN=${DOC_PIPELINE_SENTRY_DSN:-}
    volumes:
      - temp-images:/tmp/doc-pipeline
    depends_on:
      redis:
        condition: service_healthy
      inference-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # Worker DocID 4 — stateless queue consumer (no GPU)
  worker-docid-4:
    build:
      context: .
      dockerfile: Dockerfile.worker
    container_name: doc-pipeline-worker-docid-4
    ports:
      - "9016:9010"
    environment:
      - WORKER_ID=docid-4
      - DOC_PIPELINE_REDIS_URL=redis://redis:6379/0
      - DOC_PIPELINE_LOG_JSON=true
      - DOC_PIPELINE_LOG_LEVEL=INFO
      - DOC_PIPELINE_WORKER_HEALTH_PORT=9010
      - DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS=${DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS:-120}
      - DOC_PIPELINE_WORKER_CONCURRENT_JOBS=${DOC_PIPELINE_WORKER_CONCURRENT_JOBS:-4}
      - DOC_PIPELINE_SENTRY_DSN=${DOC_PIPELINE_SENTRY_DSN:-}
    volumes:
      - temp-images:/tmp/doc-pipeline
    depends_on:
      redis:
        condition: service_healthy
      inference-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # Worker DocID 5 — stateless queue consumer (no GPU)
  worker-docid-5:
    build:
      context: .
      dockerfile: Dockerfile.worker
    container_name: doc-pipeline-worker-docid-5
    ports:
      - "9018:9010"
    environment:
      - WORKER_ID=docid-5
      - DOC_PIPELINE_REDIS_URL=redis://redis:6379/0
      - DOC_PIPELINE_LOG_JSON=true
      - DOC_PIPELINE_LOG_LEVEL=INFO
      - DOC_PIPELINE_WORKER_HEALTH_PORT=9010
      - DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS=${DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS:-120}
      - DOC_PIPELINE_WORKER_CONCURRENT_JOBS=${DOC_PIPELINE_WORKER_CONCURRENT_JOBS:-4}
      - DOC_PIPELINE_SENTRY_DSN=${DOC_PIPELINE_SENTRY_DSN:-}
    volumes:
      - temp-images:/tmp/doc-pipeline
    depends_on:
      redis:
        condition: service_healthy
      inference-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # OCR Worker — stateless queue consumer (no GPU)
  worker-ocr:
    build:
      context: .
      dockerfile: Dockerfile.worker
    ports:
      - "9011:9011"
    environment:
      - DOC_PIPELINE_REDIS_URL=redis://redis:6379/0
      - DOC_PIPELINE_LOG_JSON=true
      - DOC_PIPELINE_LOG_LEVEL=INFO
      - DOC_PIPELINE_WORKER_OCR_HEALTH_PORT=9011
      - DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS=${DOC_PIPELINE_INFERENCE_TIMEOUT_SECONDS:-120}
      - DOC_PIPELINE_SENTRY_DSN=${DOC_PIPELINE_SENTRY_DSN:-}
    volumes:
      - temp-images:/tmp/doc-pipeline
    command: ["python", "worker_ocr.py"]
    depends_on:
      redis:
        condition: service_healthy
      inference-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9011/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # Promtail — log collector for doc-pipeline containers
  promtail:
    image: grafana/promtail:latest
    container_name: doc-pipeline-promtail
    volumes:
      - ./promtail-config.yaml:/etc/promtail/config.yaml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - promtail-positions:/promtail
    command: -config.file=/etc/promtail/config.yaml
    restart: unless-stopped

volumes:
  redis-data:
  temp-images:
  model-cache:
  easyocr-cache:
  promtail-positions:

networks:
  default:
    name: doc-pipeline
